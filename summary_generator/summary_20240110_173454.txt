Introduction In information societies, operations, decisions and choices previously left to humans are increasingly delegated to algorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken as a result.1 Examples abound. Online service providers continue to mediate how information is accessed with personalisation and filtering algorithms (Newell and Marabelli, 2015; Taddeo and Floridi, 2015). Together, the map and review provide a common structure for future discussion of the ethics of algorithms. In the final section of the paper we assess the fit between the proposed map and themes currently raised in the reviewed literature to identify areas of the ‘ethics of algorithms’ requiring further research. Public discourse is currently dominated by concerns with a particular class of algorithms that make decisions, e.g. the best action to take in a given situation, the best interpretation of data, and so on. Machine learning is defined by the capacity to define or modify decision-making rules autonomously. As a result, tasks performed by machine learning are difficult to predict beforehand Mittelstadt et al. 3 (how a new input will be handled) or explain afterwards (how a particular decision was made). Map of the ethics of algorithms Using the key terms defined in the previous section, we propose a conceptual map (Figure 1) based on six types of concerns that are jointly sufficient for a principled organisation of the field, and conjecture that it allows for a more rigorous diagnosis of ethical challenges related to the use of algorithms. The map is not proposed from a particular theoretical or methodological approach to ethics, but rather is intended as a prescriptive framework of types of issues arising from algorithms owing to three aspects of how algorithms operate. This work is performed in ways that are complex and (semi-)autonomous, which (3) complicates apportionment of responsibility for effects of actions driven by algorithms. The map is thus not intended as a tool to help solve ethical dilemmas arising from problematic actions driven by algorithms, but rather is posed as an organising structure based on how algorithms operate that can structure future discussion of ethical issues. Inscrutable evidence When data are used as (or processed to produce) evidence for a conclusion, it is reasonable to expect that the connection between the data and the conclusion should be accessible (i.e. intelligible as well as open to scrutiny and perhaps even critique).7 When the connection is not obvious, this expectation can be satisfied by better access as well as by additional explanations. To assess the utility of the map, and to observe how each of these kinds of concerns manifests in ethical problems already observed in algorithms, a systematic review of academic literature was carried out.9 The following sections (4 to 10) describe how ethical issues and concepts are treated in the literature explicitly discussing the ethical aspects of algorithms. Acting on correlations can be doubly problematic.10 Spurious correlations may be discovered rather than genuine causal knowledge. Burrell (2016) and Schermer (2011) argue that the opacity of machine learning algorithms inhibits oversight. Algorithms ‘‘are opaque in the sense that if one is a recipient of the output of the algorithm (the classification decision), rarely does one have any concrete sense of how or why a particular classification has been arrived at from inputs’’ (Burrell, 2016: 1). Matthias (2004: 179) suggests that machine learning can produce outputs for which ‘‘the human trainer himself is unable to provide an algorithmic representation.’’ Algorithms can only be considered explainable to the degree that a human can articulate the trained model or rationale of a particular decision, for instance by explaining the (quantified) influence of particular inputs or attributes (Datta et al., 2016). Meaningful oversight and human intervention in algorithmic decision-making ‘‘is impossible when the machine has an informational advantage over the operator ... [or] when the machine cannot be controlled by a human in real-time due to its processing speed and the multitude of operational variables’’ (Matthias, 2004: 182– 183). Unfair outcomes leading to discrimination Much of the reviewed literature also addresses how discrimination results from biased evidence and decisionmaking.15 Profiling by algorithms, broadly defined ‘‘as the construction or inference of patterns by means of data mining and ... the application of the ensuing profiles to people whose data match with them’’ (Hildebrandt and Koops, 2010: 431), is frequently cited as a source of discrimination. It may be possible to direct algorithms not to consider sensitive attributes that contribute to discrimination (Barocas and Selbst, 2015), such as gender or ethnicity (Calders et al., 2009; Kamiran and Calders, 2010; Schermer, 2011), based upon the emergence of discrimination in a particular context. Personalisation through non-distributive profiling, seen for example in personalised pricing in insurance premiums (Hildebrandt and Koops, 2010; Van Wel and Royakkers, 2004), can be discriminatory by violating both ethical and legal principles of equal or fair treatment of individuals (Newell and Marabelli, 2015). Transformative effects leading to challenges for autonomy Value-laden decisions made by algorithms can also pose a threat to the autonomy of data subjects. Classifications and streams of behavioural data are used to match information to the interests and attributes of data subjects. Responses to discrimination, de-individualisation and the threats of opaque decision-making for data subjects’ agency often appeal to informational privacy (Schermer, 2011), or the right of data subjects to ‘‘shield personal data from third parties.’’ Informational privacy concerns the capacity of an individual to control information about herself (Van Wel Mittelstadt et al. 9 and Royakkers, 2004), and the effort required by third parties to obtain this information. The ‘identifiable individual’ is not necessarily a part of these processes. Fule and Roddick (2004: 159) suggest operators also have a responsibility to monitor for ethical impacts of decision-making by algorithms because ‘‘the sensitivity of a rule may not be apparent to the miner ... the ability to harm or to cause offense can often be inadvertent.’’ Schermer (2011) similarly suggests that data processors should actively searching for errors and bias in their 10 Big Data & Society algorithms and models. Human oversight of complex systems as an accountability mechanism may, however, be impossible due to the challenges for transparency already mentioned (see ‘Inscrutable evidence leading to opacity’ section). For some, learning algorithms should be considered moral agents with some degree of moral responsibility. Points of further research As the preceding discussion demonstrates, the proposed map (see ‘Map of the ethics of algorithms’ section) can be used to organise current academic discourse describing ethical concerns with algorithms in a principled way, on purely epistemic and ethical grounds. As new types of ethical concerns with algorithms are identified, or if one of the six described types can be separated into two or more types, the map can be revised. Concerning transformative effects, algorithms change how identity is constructed, managed and protected by privacy and data protection mechanisms (see ‘Transformative effects leading to challenges for informational privacy’ section). Real world mechanisms to enforce privacy in analytics are also required. First, despite a wealth of literature addressing the moral responsibility and agency of algorithms, insufficient attention has been given to distributed responsibility, or responsibility as shared across a network of human and algorithmic actors simultaneously (cf. The reviewed literature (see ‘Traceability leading to moral responsibility’ section) addresses the potential moral agency of algorithms, but does not describe methods and principles for apportioning blame or responsibility across a mixed network of human and algorithmic actors. Further work is also required to specify requirements for resilience to malfunctioning as an ethical ideal in algorithm design. Adler et al., 2016; Sandvig et al., 2014) that build upon current work in transparency and interpretability of machine learning (e.g. Kim et al., 2015; Lou et al., 2013). 35(3)(a)). 21) and a right not to be subject to solely automated processed individual decision-making23 (Art. Practical requirements will need to be unpacked in the future that strike an appropriate balance between data subjects’ rights to be informed about the logic and consequences of profiling, and the burden imposed on data controllers. The work described here has made three contributions to clarify the ethical importance of this mediation: (1) a review of existing discussion of ethical aspects of algorithms; (2) a prescriptive map to organise discussion; and (3) a critical assessment of the literature to identify areas requiring further work to develop the ethics of algorithms. Some of these interdependencies are present in the literature we reviewed, like the connection between bias and discrimination (see ‘Misguided evidence leading to bias’ and ‘Unfair outcomes leading to discrimination’ sections) or the impact of opacity on the attribution of responsibility (see ‘Inscrutable evidence leading to opacity’ and ‘Traceability leading to moral responsibility’ sections).